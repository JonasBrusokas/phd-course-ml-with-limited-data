{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'really'], ['really', 'like'], ['like', 'living'], ['living', 'dangerously']]\n",
      "[['I', 'really', 'like'], ['really', 'like', 'living'], ['like', 'living', 'dangerously']]\n",
      "[['I', 'really', 'like', 'living'], ['really', 'like', 'living', 'dangerously']]\n"
     ]
    }
   ],
   "source": [
    "# N-grams\n",
    "\n",
    "# N-gram = unique N-plet appearing in a corpus\n",
    "\n",
    "# Example\n",
    "\n",
    "sentence = \"I really like living dangerously\"\n",
    "words = sentence.split(' ')\n",
    "\n",
    "def calculate_n_gram(words, n):\n",
    "    start = 0\n",
    "    end = n\n",
    "    ngrams = []\n",
    "    while (end-1) < len(words):\n",
    "        ngrams.append(words[start: end])\n",
    "        start += 1\n",
    "        end += 1\n",
    "    print(ngrams)\n",
    "    return ngrams\n",
    "\n",
    "grams_2 = calculate_n_gram(words, 2)\n",
    "grams_3 = calculate_n_gram(words, 3)\n",
    "grams_4 = calculate_n_gram(words, 4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T14:34:44.948538184Z",
     "start_time": "2023-09-25T14:34:44.904644052Z"
    }
   },
   "id": "d8c29b21e150e50"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Probability of P(happy | I am) = C(\"I am\"(w^2_1) happy(w_3) ) / C (I am(w^2_1) )\n",
    "\n",
    "# Markov assumption: previous N state(s) encapsulates the entire chain of events PRIOR\n",
    "# Unnecessary to calculate the entire chain \n",
    "\n",
    "# This assumption operates this way:\n",
    "# P(\"I am ready\") = P (\"I\" | start) * P (\"am\" | \"I\") * P (\"ready\" | am)\n",
    "\n",
    "# logarithms can help here!\n",
    "# we can do log(p1) + log(p2) + log(p3) instead!\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T14:34:46.031949116Z",
     "start_time": "2023-09-25T14:34:46.005333967Z"
    }
   },
   "id": "4a9f049e68c8f410"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perplexity: PP(W) - measure of complexity\n",
    "\n",
    "# Intuition: how likely that this text is written by humans (similar to source)\n",
    "# Perplexity is closely related to \"entropy\"\n",
    "\n",
    "# Lower perplexity = more human, more regular => lower equals better!\n",
    "# log PP(W) is much easier to compute!\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a3ae97be5acda38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#<start> <stop> <unknown>\n",
    "# We can limit size for vocabulary, but we should be careful"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9949303829734825"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Smoothing helps, as it removes n-grams from becoming completely impossible === P(*) = 0\n",
    "\n",
    "# Alternative, we can do backoff. If n=3, our 3-gram might not have a specific probability for a sequence. But we can then look for a 2-gram or 1-gram, until we find P(*) != 0\n",
    "# Stupid backoff = use N-1gram * 0.4 (constant, that is selected apriori)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed50b726c5e8f8de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

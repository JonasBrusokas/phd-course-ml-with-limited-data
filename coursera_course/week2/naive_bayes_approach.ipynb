{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-24T21:12:22.369710953Z",
     "start_time": "2023-09-24T21:12:22.363101155Z"
    }
   },
   "outputs": [],
   "source": [
    "# triples of 'word', pos count, neg count\n",
    "dataset = [\n",
    "    ('I', 3, 3),\n",
    "    ('am', 3, 3),\n",
    "    ('happy', 2, 1),\n",
    "    ('because', 1, 0),\n",
    "    ('learning', 1, 1),\n",
    "    ('nlp', 1, 1),\n",
    "    ('sad', 1, 2),\n",
    "    ('not', 1, 2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(13, 13)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pos = sum(map(lambda triple: triple[1], dataset))\n",
    "n_neg = sum(map(lambda triple: triple[2], dataset))\n",
    "\n",
    "n_pos, n_neg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T21:12:27.522037979Z",
     "start_time": "2023-09-24T21:12:27.514379246Z"
    }
   },
   "id": "e571a1cbc967c9b5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[('I', 0.23076923076923078, 0.23076923076923078),\n ('am', 0.23076923076923078, 0.23076923076923078),\n ('happy', 0.15384615384615385, 0.07692307692307693),\n ('because', 0.07692307692307693, 0.0),\n ('learning', 0.07692307692307693, 0.07692307692307693),\n ('nlp', 0.07692307692307693, 0.07692307692307693),\n ('sad', 0.07692307692307693, 0.15384615384615385),\n ('not', 0.07692307692307693, 0.15384615384615385)]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating P(word_i | class)\n",
    "\n",
    "dataset_prob = []\n",
    "\n",
    "for triple in dataset:\n",
    "    pos_prob = triple[1] / n_pos\n",
    "    neg_prob = triple[2] / n_neg\n",
    "    dataset_prob.append((triple[0], pos_prob, neg_prob))\n",
    "\n",
    "dataset_prob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T21:14:54.253932554Z",
     "start_time": "2023-09-24T21:14:54.245458616Z"
    }
   },
   "id": "a3412e5ade9315eb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('I', 0.19047619047619047, 0.19047619047619047),\n ('am', 0.19047619047619047, 0.19047619047619047),\n ('happy', 0.14285714285714285, 0.09523809523809523),\n ('because', 0.09523809523809523, 0.047619047619047616),\n ('learning', 0.09523809523809523, 0.09523809523809523),\n ('nlp', 0.09523809523809523, 0.09523809523809523),\n ('sad', 0.09523809523809523, 0.14285714285714285),\n ('not', 0.09523809523809523, 0.14285714285714285)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Laplacian Smoothing\n",
    "\n",
    "dataset_prob_lap = []\n",
    "\n",
    "v_class = len(dataset) # Number of words\n",
    "print(v_class)\n",
    "\n",
    "for triple in dataset:\n",
    "    pos_prob = (triple[1] + 1) / (n_pos + v_class)\n",
    "    neg_prob = (triple[2] + 1)/ (n_neg + v_class)\n",
    "    dataset_prob_lap.append((triple[0], pos_prob, neg_prob))\n",
    "\n",
    "dataset_prob_lap"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T21:17:42.530949809Z",
     "start_time": "2023-09-24T21:17:42.489375807Z"
    }
   },
   "id": "30fe4a47f518b923"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[('I', 1.0),\n ('am', 1.0),\n ('happy', 1.5),\n ('because', 2.0),\n ('learning', 1.0),\n ('nlp', 1.0),\n ('sad', 0.6666666666666666),\n ('not', 0.6666666666666666)]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log likelihood - logarithms of probabilities\n",
    "\n",
    "# Sentiments: Positive | Neutral | Negative\n",
    "\n",
    "def ratio(triple):\n",
    "    # Frequency positive + 1/ Frequency negative + 1\n",
    "    # NOTE: we need Laplacian smoothing, otherwise we can get division by zero \n",
    "    P_pos = triple[1]\n",
    "    P_neg = triple[2]\n",
    "    return triple[0], P_pos / P_neg\n",
    "\n",
    "\n",
    "ratios = list(map(lambda triple: ratio(triple), dataset_prob_lap))\n",
    "ratios\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T21:22:08.746992596Z",
     "start_time": "2023-09-24T21:22:08.703082381Z"
    }
   },
   "id": "1a8f39cbde335333"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Classification with Naive Bayes\n",
    "\n",
    "# P_pos / P_neg (prior)\n",
    "# product = 1\n",
    "# for ratio in ratios\n",
    "#   product *= ratio\n",
    "\n",
    "# if product > 1 -> positive\n",
    "\n",
    "# Numerical underflow can be avoided with log likelihood\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e9417a2c90c4f90"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'am', 'I', 'learning', 'because']\n",
      "1.0986122886681096\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# Classify one tweet\n",
    "\n",
    "tweet = \"I am happy because I am learning\"\n",
    "words = tweet.split(\" \")\n",
    "unique_words = list(set(words))\n",
    "\n",
    "print(unique_words)\n",
    "\n",
    "def get_word_in_tuple_dict(key, tuple_dict):\n",
    "    for tuple in tuple_dict:\n",
    "        if (key == tuple[0]):\n",
    "            return tuple[1:]\n",
    "    return None\n",
    "\n",
    "# get_word_in_tuple_dict('I', dataset_prob_lap)\n",
    "\n",
    "import math\n",
    "\n",
    "# inference result will be a sum of log ratios\n",
    "prior = n_pos / n_neg\n",
    "# We add prior immediately, is important for imbalanced datasets\n",
    "inference_result = 0 + math.log(prior)\n",
    "for word in unique_words:\n",
    "    ratio = get_word_in_tuple_dict(word, ratios)\n",
    "    inference_result += math.log(ratio[0])\n",
    "    \n",
    "inference_result\n",
    "\n",
    "print(inference_result)\n",
    "if (inference_result > 0): # NOTE: >0, not >1, because these are log values\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T21:33:20.699939210Z",
     "start_time": "2023-09-24T21:33:20.651027561Z"
    }
   },
   "id": "3c054db2412c4c7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
